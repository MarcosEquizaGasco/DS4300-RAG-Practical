{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest_redis import *\n",
    "from src.ingest_chroma import *\n",
    "from src.ingest_milvus import * \n",
    "from src.search import *\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2.1\n"
     ]
    }
   ],
   "source": [
    "print(redis.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataframe to track experiment results\n",
    "cols =  ['database', 'chunk_size', 'overlap', 'clean', 'embedding', 'chunks_processed', 'time_to_process', 'used_memory_mb', 'query_time']\n",
    "results = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 500\n"
     ]
    }
   ],
   "source": [
    "with open('example_queries.txt', 'r') as file:\n",
    "\n",
    "    # Skip lines that don't contain actual queries (headers, empty lines) and extract example queries\n",
    "    queries = [line.strip() for line in file if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('##')]\n",
    "    queries = [q.split('. ', 1)[1] if '. ' in q else q for q in queries]\n",
    "    \n",
    "    # Print total count\n",
    "    print(f\"Total queries: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "db = 'redis'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # clear store before starting\n",
    "            clear_redis_store()\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            create_hnsw_index()\n",
    "            chunk_count = process_pdfs_redis(\"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "\n",
    "            # Get memory usage info\n",
    "            memory_info = redis_client.info('memory')\n",
    "            used_memory = memory_info['used_memory'] \n",
    "            used_memory_mb = used_memory / (1024 * 1024)\n",
    "\n",
    "            # test retrieval speed\n",
    "            start = time.time()\n",
    "            for query in queries:\n",
    "                query_redis(query)\n",
    "            to_search = time.time()- start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear store before starting\n",
    "clear_redis_store()\n",
    "\n",
    "# create and fill redis store\n",
    "start = time.time()\n",
    "create_hnsw_index()\n",
    "chunk_count = process_pdfs_redis(\"data/\", chunk_size=500, overlap=100, clean = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "embedding_size = 768\n",
    "db = 'chroma'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            collection = create_chroma_index(embedding_model)\n",
    "            collection, chunk_count = process_pdfs_chroma(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "            num_vectors = len(collection.get()[\"ids\"])  # Number of stored items\n",
    "            embedding_size = 768  # Adjust based on your embedding model\n",
    "            float_size = np.dtype(np.float32).itemsize  # 4 bytes per float\n",
    "\n",
    "            memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "            memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "            print(f\"It uses {memory_usage_bytes} MB\")\n",
    "\n",
    "            # test searching speed\n",
    "            start = time.time()\n",
    "            query_chroma(collection, queries)\n",
    "            to_search = time.time()-start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection with chunk size 100 and overlap 0 created in 10.15 seconds\n",
      "It uses 1274880 MB\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for EmbeddingsRequest\nprompt\n  Input should be a valid string [type=string_type, input_value=['What is a relational da... parallelization work?'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# test searching speed\u001b[39;00m\n\u001b[32m     29\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mquery_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m to_search = time.time()-start\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSearch with chunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and overlap \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverlap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(to_search,\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Public/DS_4300/DS4300-RAG-Practical/src/ingest_milvus.py:56\u001b[39m, in \u001b[36mquery_milvus\u001b[39m\u001b[34m(collection, query_text, top_k)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_milvus\u001b[39m(collection, query_text, top_k=\u001b[32m5\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     embedding = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     search_params = {\u001b[33m\"\u001b[39m\u001b[33mmetric_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCOSINE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mef\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m100\u001b[39m}}\n\u001b[32m     58\u001b[39m     results = collection.search([embedding], \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m, search_params, top_k=top_k, output_fields=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Public/DS_4300/DS4300-RAG-Practical/src/ingest_redis.py:49\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text, model)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_embedding\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mnomic-embed-text\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/prac_2_env/lib/python3.13/site-packages/ollama/_client.py:384\u001b[39m, in \u001b[36mClient.embeddings\u001b[39m\u001b[34m(self, model, prompt, options, keep_alive)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membeddings\u001b[39m(\n\u001b[32m    371\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    372\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    375\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    376\u001b[39m ) -> EmbeddingsResponse:\n\u001b[32m    377\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m  Deprecated in favor of `embed`.\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    380\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    381\u001b[39m     EmbeddingsResponse,\n\u001b[32m    382\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    383\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/embeddings\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     json=\u001b[43mEmbeddingsRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    390\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/prac_2_env/lib/python3.13/site-packages/pydantic/main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for EmbeddingsRequest\nprompt\n  Input should be a valid string [type=string_type, input_value=['What is a relational da... parallelization work?'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type"
     ]
    }
   ],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "embedding_size = 768\n",
    "db = 'milvus'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            collection = create_milvus_collection(embed_model = embedding_model)\n",
    "            collection, chunk_count = process_pdfs_milvus(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Collection with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "            # get number\n",
    "            num_vectors = collection.num_entities \n",
    "            embedding_size = 768  \n",
    "            float_size = np.dtype(np.float32).itemsize  \n",
    "\n",
    "            memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "            memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "            print(f\"It uses {memory_usage_bytes} MB\")\n",
    "\n",
    "            # test searching speed\n",
    "            start = time.time()\n",
    "            query_milvus(collection, queries)\n",
    "            to_search = time.time()-start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['database']=='chroma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['database']=='chroma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index().loc[35, 'chunks_processed'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('experiment_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS4300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
