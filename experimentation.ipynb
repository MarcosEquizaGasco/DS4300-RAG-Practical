{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest_redis import *\n",
    "from src.ingest_chroma import *\n",
    "from src.ingest_milvus import * \n",
    "from src.search import *\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pymilvus import connections, utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataframe to track experiment results\n",
    "cols =  ['database', 'chunk_size', 'overlap', 'clean', 'embedding', 'chunks_processed', 'time_to_process', 'used_memory_mb', 'query_time']\n",
    "results = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 500\n"
     ]
    }
   ],
   "source": [
    "with open('example_queries.txt', 'r') as file:\n",
    "\n",
    "    # Skip lines that don't contain actual queries (headers, empty lines) and extract example queries\n",
    "    queries = [line.strip() for line in file if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('##')]\n",
    "    queries = [q.split('. ', 1)[1] if '. ' in q else q for q in queries]\n",
    "    \n",
    "    # Print total count\n",
    "    print(f\"Total queries: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 10.43 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 9.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/c154mt9s11x5q965j91l06qw0000gn/T/ipykernel_7749/121440660.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 10.19 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 9.45 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 11.47 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 11.94 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 15.37 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 11.93 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 15.41 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 11.49 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 15.34 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 10.43 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 21.64 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 12.12 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 25.04 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 12.15 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 23.0 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 11.9 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 24.02 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 10.92 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 22.97 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 10.98 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 24.92 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 11.43 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 10.01 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 11.95 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 9.99 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 10.92 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 10.89 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 10.97 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 12.24 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 10.59 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 12.41 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 10.71 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 12.58 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 10.03 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 9.52 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 10.56 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 9.43 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 10.72 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 11.89 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 10.01 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 12.5 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 10.31 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 12.27 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 10.34 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 12.69 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 9.97 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 9.71 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 10.74 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 10.33 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 10.66 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 12.6 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 9.66 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 12.35 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 9.4 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 12.55 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 10.4 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 13.44 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 9.83 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 9.1 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 11.33 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 9.45 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 11.24 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 10.92 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 10.61 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 11.77 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 10.64 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 10.69 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 9.77 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 11.96 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 10.91 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 9.41 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 11.33 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 9.35 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 11.44 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 11.54 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 11.06 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 11.49 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 10.91 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 11.77 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 9.85 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 10.93 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 9.26 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 9.35 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 11.49 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 9.42 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 10.44 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 10.75 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 10.48 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 12.03 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 10.23 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 11.7 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 9.12 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 10.2 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 9.72 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 9.01 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 11.54 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 9.39 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 11.55 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 10.71 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 9.83 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 8.88 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 8.21 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 9.89 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 9.65 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 10.98 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 11.32 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 9.16 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 11.47 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 9.49 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 10.11 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 8.75 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 9.01 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 10.44 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 10.15 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 11.5 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 8.42 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 9.79 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 8.31 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 6.87 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.91 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 7.82 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.69 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 8.26 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.1 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 8.15 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.94 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 9.3 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.1 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 9.49 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_models = {'nomic-embed-text':768, 'mxbai-embed-large':512, 'bge-m3':1024}\n",
    "db = 'redis'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        for embedding_model, embed_size in embedding_models.items():\n",
    "\n",
    "            if overlap >= chunk:\n",
    "                continue\n",
    "            \n",
    "            for clean in [True, False]:\n",
    "\n",
    "                # clear store before starting\n",
    "                clear_redis_store()\n",
    "\n",
    "                # create and fill redis store\n",
    "                start = time.time()\n",
    "                create_hnsw_index()\n",
    "                chunk_count = process_pdfs_redis(\"data/\", chunk_size=chunk, overlap=overlap, clean = clean, model=embedding_model)\n",
    "                to_fill = time.time() - start\n",
    "                print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "\n",
    "                # Get memory usage info\n",
    "                memory_info = redis_client.info('memory')\n",
    "                used_memory = memory_info['used_memory'] \n",
    "                used_memory_mb = used_memory / (1024 * 1024)\n",
    "\n",
    "                # test retrieval speed\n",
    "                start = time.time()\n",
    "                for query in queries:\n",
    "                    query_redis(query)\n",
    "                to_search = time.time()- start\n",
    "                print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "                # add results to result dataframe\n",
    "                new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "                results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 16.76 seconds\n",
      "It uses 1274880 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 7.62 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 14.78 seconds\n",
      "It uses 1299456 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 6.63 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 18.42 seconds\n",
      "It uses 849920 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 7.7 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 18.42 seconds\n",
      "It uses 866304 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 7.78 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 19.52 seconds\n",
      "It uses 1956864 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 7.13 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 21.87 seconds\n",
      "It uses 2055168 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 7.54 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 26.42 seconds\n",
      "It uses 1304576 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 9.59 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 29.0 seconds\n",
      "It uses 1370112 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 9.91 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 14.49 seconds\n",
      "It uses 930816 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 8.42 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 14.85 seconds\n",
      "It uses 936960 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 8.46 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 17.92 seconds\n",
      "It uses 620544 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 9.86 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 18.64 seconds\n",
      "It uses 624640 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 9.71 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 12.93 seconds\n",
      "It uses 967680 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 6.57 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 13.19 seconds\n",
      "It uses 973824 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 6.92 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 18.34 seconds\n",
      "It uses 645120 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 10.48 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 20.01 seconds\n",
      "It uses 649216 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 10.05 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 15.93 seconds\n",
      "It uses 1010688 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 8.99 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 16.26 seconds\n",
      "It uses 1019904 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 7.58 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 17.81 seconds\n",
      "It uses 673792 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 10.32 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 19.27 seconds\n",
      "It uses 679936 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 7.55 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 11.78 seconds\n",
      "It uses 890880 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 7.2 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 12.53 seconds\n",
      "It uses 890880 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 7.66 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 16.46 seconds\n",
      "It uses 593920 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 9.73 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 17.72 seconds\n",
      "It uses 593920 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 10.17 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 13.44 seconds\n",
      "It uses 897024 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 7.4 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 11.39 seconds\n",
      "It uses 900096 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 5.86 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 15.38 seconds\n",
      "It uses 598016 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 7.86 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 15.79 seconds\n",
      "It uses 600064 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 7.67 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 11.91 seconds\n",
      "It uses 909312 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 7.76 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 12.08 seconds\n",
      "It uses 909312 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 7.1 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 17.31 seconds\n",
      "It uses 606208 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 10.3 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 16.25 seconds\n",
      "It uses 606208 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 8.82 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 12.34 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 7.15 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 12.79 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 6.63 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 15.1 seconds\n",
      "It uses 569344 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 8.71 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 16.07 seconds\n",
      "It uses 569344 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 7.74 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 12.6 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 8.81 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 13.72 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 8.61 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 15.45 seconds\n",
      "It uses 569344 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 7.52 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 15.17 seconds\n",
      "It uses 569344 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 7.79 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 12.16 seconds\n",
      "It uses 857088 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 7.1 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 12.71 seconds\n",
      "It uses 860160 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 6.93 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 15.21 seconds\n",
      "It uses 571392 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.03 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 16.12 seconds\n",
      "It uses 573440 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.64 seconds\n"
     ]
    }
   ],
   "source": [
    "db = 'chroma'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        for embedding_model, embed_size in embedding_models.items():\n",
    "\n",
    "            if embedding_model == 'bge-m3':\n",
    "                continue\n",
    "\n",
    "            if overlap >= chunk:\n",
    "                continue\n",
    "            for clean in [True, False]:\n",
    "\n",
    "                # create and fill redis store\n",
    "                start = time.time()\n",
    "                collection = create_chroma_index(embedding_model)\n",
    "                collection, chunk_count = process_pdfs_chroma(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "                to_fill = time.time() - start\n",
    "                print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "                num_vectors = len(collection.get()[\"ids\"])  # Number of stored items\n",
    "                embedding_size = embed_size\n",
    "                float_size = np.dtype(np.float32).itemsize  # 4 bytes per float\n",
    "\n",
    "                memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "                memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "                print(f\"It uses {memory_usage_bytes} MB\")\n",
    "\n",
    "                # test searching speed\n",
    "                start = time.time()\n",
    "                query_chroma(collection, queries)\n",
    "                to_search = time.time()-start\n",
    "                print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "                # add results to result dataframe\n",
    "                new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, memory_usage_mb, to_search]\n",
    "                results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection with chunk size 100 and overlap 0 created in 13.7 seconds\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Collection' object has no attribute 'get_collection_stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCollection with chunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and overlap \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverlap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m created in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(to_fill,\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# get memory usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m stats = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection_stats\u001b[49m()\n\u001b[32m     21\u001b[39m stats_dict = ast.literal_eval(stats)\n\u001b[32m     23\u001b[39m memory_bytes = \u001b[32m0\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Collection' object has no attribute 'get_collection_stats'"
     ]
    }
   ],
   "source": [
    "db = 'milvus'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        for embedding_model, embed_size in embedding_models.items():\n",
    "\n",
    "            if overlap >= chunk:\n",
    "                continue\n",
    "            for clean in [True, False]:\n",
    "\n",
    "                # create and fill redis store\n",
    "                start = time.time()\n",
    "                collection = create_milvus_collection(embed_model = embedding_model)\n",
    "                collection, chunk_count = process_pdfs_milvus(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "                to_fill = time.time() - start\n",
    "                print(f'Collection with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "                # # get memory usage\n",
    "                # stats = collection.get_collection_stats()\n",
    "                # stats_dict = ast.literal_eval(stats)\n",
    "                \n",
    "                # memory_bytes = 0\n",
    "                # if \"segments_stat\" in stats_dict:\n",
    "                #     for segment in stats_dict[\"segments_stat\"]:\n",
    "                #         if \"memory_usage\" in segment:\n",
    "                #             memory_bytes += int(segment[\"memory_usage\"])\n",
    "                \n",
    "                # # Convert to MB\n",
    "                # memory_mb = memory_bytes / (1024 * 1024)\n",
    "\n",
    "                # index_info = collection.get_index_info()\n",
    "    \n",
    "                # # Calculate total index size\n",
    "                # total_size_bytes = 0\n",
    "                # for info in index_info:\n",
    "                #     if \"index_size\" in info:\n",
    "                #         total_size_bytes += int(info[\"index_size\"])\n",
    "                \n",
    "                # # Convert to MB\n",
    "                # total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "                print(f\"It uses {total_size_mb} MB\")\n",
    "\n",
    "                # test searching speed\n",
    "                start = time.time()\n",
    "                query_milvus(collection, queries)\n",
    "                to_search = time.time()-start\n",
    "                print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "                # add results to result dataframe\n",
    "                new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "                results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prac_2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
