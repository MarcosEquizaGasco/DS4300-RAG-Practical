{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest_redis import *\n",
    "from src.ingest_chroma import *\n",
    "from src.ingest_milvus import * \n",
    "from src.search import *\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataframe to track experiment results\n",
    "cols =  ['database', 'chunk_size', 'overlap', 'clean', 'embedding', 'chunks_processed', 'time_to_process', 'used_memory_mb', 'query_time']\n",
    "results = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 500\n"
     ]
    }
   ],
   "source": [
    "with open('example_queries.txt', 'r') as file:\n",
    "\n",
    "    # Skip lines that don't contain actual queries (headers, empty lines) and extract example queries\n",
    "    queries = [line.strip() for line in file if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('##')]\n",
    "    queries = [q.split('. ', 1)[1] if '. ' in q else q for q in queries]\n",
    "    \n",
    "    # Print total count\n",
    "    print(f\"Total queries: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 9.11 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 9.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/t_5vbv0n4_j3m_7gy69b59tm0000gn/T/ipykernel_52017/933467114.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 11.61 seconds\n",
      "Search with chunk size 100 and overlap 0 completed in 9.71 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 17.04 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 11.14 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 18.03 seconds\n",
      "Search with chunk size 100 and overlap 50 completed in 10.03 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 8.95 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 9.38 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 10.15 seconds\n",
      "Search with chunk size 300 and overlap 0 completed in 9.4 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 9.33 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 9.08 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 10.15 seconds\n",
      "Search with chunk size 300 and overlap 50 completed in 9.23 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 8.61 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 9.57 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 10.38 seconds\n",
      "Search with chunk size 300 and overlap 100 completed in 9.84 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 8.38 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 10.07 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 8.66 seconds\n",
      "Search with chunk size 500 and overlap 0 completed in 9.9 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 8.52 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 10.13 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 9.14 seconds\n",
      "Search with chunk size 500 and overlap 50 completed in 9.72 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 8.42 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 8.88 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 9.25 seconds\n",
      "Search with chunk size 500 and overlap 100 completed in 8.98 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 8.41 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 9.37 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 8.92 seconds\n",
      "Search with chunk size 1000 and overlap 0 completed in 9.43 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 8.34 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 9.41 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 9.12 seconds\n",
      "Search with chunk size 1000 and overlap 50 completed in 9.29 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 8.66 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 9.52 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 9.59 seconds\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "db = 'redis'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # clear store before starting\n",
    "            clear_redis_store()\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            create_hnsw_index()\n",
    "            chunk_count = process_pdfs_redis(\"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "\n",
    "            # Get memory usage info\n",
    "            memory_info = redis_client.info('memory')\n",
    "            used_memory = memory_info['used_memory'] \n",
    "            used_memory_mb = used_memory / (1024 * 1024)\n",
    "            print(f\"It uses {used_memory_mb} MB\")\n",
    "\n",
    "            # test retrieval speed\n",
    "            start = time.time()\n",
    "            for query in queries:\n",
    "                query_redis(query)\n",
    "            to_search = time.time()- start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 14.7 seconds\n",
      "It uses 1274880 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 8.57 seconds\n",
      "Index with chunk size 100 and overlap 0 created in 15.95 seconds\n",
      "It uses 1299456 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 8.75 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 21.35 seconds\n",
      "It uses 1956864 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 8.34 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 22.63 seconds\n",
      "It uses 2055168 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 8.27 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 14.5 seconds\n",
      "It uses 930816 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 8.59 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 14.35 seconds\n",
      "It uses 936960 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 7.41 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 12.63 seconds\n",
      "It uses 967680 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 9.04 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 15.09 seconds\n",
      "It uses 973824 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 8.51 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 14.94 seconds\n",
      "It uses 1010688 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 9.22 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 15.63 seconds\n",
      "It uses 1019904 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 9.41 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 13.93 seconds\n",
      "It uses 890880 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 8.71 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 13.17 seconds\n",
      "It uses 890880 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 7.85 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 12.57 seconds\n",
      "It uses 897024 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 8.25 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 19.62 seconds\n",
      "It uses 900096 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 9.66 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 12.09 seconds\n",
      "It uses 909312 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 7.69 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 13.97 seconds\n",
      "It uses 909312 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 8.3 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 13.85 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 8.94 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 14.92 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 8.57 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 13.88 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 8.88 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 13.99 seconds\n",
      "It uses 854016 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 8.61 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 12.59 seconds\n",
      "It uses 857088 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.48 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 13.96 seconds\n",
      "It uses 860160 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 8.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "embedding_size = 768\n",
    "db = 'chroma'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # create and fill chroma store\n",
    "            start = time.time()\n",
    "            collection = create_chroma_index(embedding_model)\n",
    "            collection, chunk_count = process_pdfs_chroma(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "            num_vectors = len(collection.get()[\"ids\"])  # Number of stored items\n",
    "            embedding_size = 768  # Adjust based on your embedding model\n",
    "            float_size = np.dtype(np.float32).itemsize  # 4 bytes per float\n",
    "\n",
    "            memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "            memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "            print(f\"It uses {memory_usage_bytes} MB\")\n",
    "\n",
    "            # test searching speed\n",
    "            start = time.time()\n",
    "            query_chroma(collection, queries)\n",
    "            to_search = time.time()-start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, memory_usage_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 12.19 seconds\n",
      "It uses 1.22 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 17.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/t_5vbv0n4_j3m_7gy69b59tm0000gn/T/ipykernel_55250/3922220688.py:35: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index with chunk size 100 and overlap 0 created in 13.81 seconds\n",
      "It uses 1.24 MB\n",
      "Search with chunk size 100 and overlap 0 completed in 17.19 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 15.13 seconds\n",
      "It uses 1.87 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 17.26 seconds\n",
      "Index with chunk size 100 and overlap 50 created in 17.39 seconds\n",
      "It uses 1.96 MB\n",
      "Search with chunk size 100 and overlap 50 completed in 16.32 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 10.61 seconds\n",
      "It uses 0.89 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 16.88 seconds\n",
      "Index with chunk size 300 and overlap 0 created in 12.24 seconds\n",
      "It uses 0.89 MB\n",
      "Search with chunk size 300 and overlap 0 completed in 15.84 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 11.04 seconds\n",
      "It uses 0.92 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 17.13 seconds\n",
      "Index with chunk size 300 and overlap 50 created in 11.34 seconds\n",
      "It uses 0.93 MB\n",
      "Search with chunk size 300 and overlap 50 completed in 16.84 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 12.89 seconds\n",
      "It uses 0.96 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 17.15 seconds\n",
      "Index with chunk size 300 and overlap 100 created in 12.15 seconds\n",
      "It uses 0.97 MB\n",
      "Search with chunk size 300 and overlap 100 completed in 17.39 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 11.18 seconds\n",
      "It uses 0.85 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 16.42 seconds\n",
      "Index with chunk size 500 and overlap 0 created in 11.0 seconds\n",
      "It uses 0.85 MB\n",
      "Search with chunk size 500 and overlap 0 completed in 16.04 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 10.6 seconds\n",
      "It uses 0.86 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 15.85 seconds\n",
      "Index with chunk size 500 and overlap 50 created in 12.17 seconds\n",
      "It uses 0.86 MB\n",
      "Search with chunk size 500 and overlap 50 completed in 17.27 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 11.77 seconds\n",
      "It uses 0.87 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 16.91 seconds\n",
      "Index with chunk size 500 and overlap 100 created in 12.02 seconds\n",
      "It uses 0.87 MB\n",
      "Search with chunk size 500 and overlap 100 completed in 17.18 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 10.23 seconds\n",
      "It uses 0.81 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 16.64 seconds\n",
      "Index with chunk size 1000 and overlap 0 created in 12.04 seconds\n",
      "It uses 0.81 MB\n",
      "Search with chunk size 1000 and overlap 0 completed in 17.21 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 11.67 seconds\n",
      "It uses 0.81 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 15.96 seconds\n",
      "Index with chunk size 1000 and overlap 50 created in 10.21 seconds\n",
      "It uses 0.81 MB\n",
      "Search with chunk size 1000 and overlap 50 completed in 15.65 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 11.35 seconds\n",
      "It uses 0.82 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 16.63 seconds\n",
      "Index with chunk size 1000 and overlap 100 created in 11.3 seconds\n",
      "It uses 0.82 MB\n",
      "Search with chunk size 1000 and overlap 100 completed in 16.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "embedding_size = 768\n",
    "db = 'milvus'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # create and fill Milvus collection\n",
    "            start = time.time()\n",
    "            collection = create_milvus_collection(embed_dim=embedding_size)\n",
    "            collection, num_vectors = process_pdfs_milvus(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean=clean)\n",
    "            index_time = time.time() - start\n",
    "            print(f\"Index with chunk size {chunk} and overlap {overlap} created in {round(index_time, 2)} seconds\")\n",
    "\n",
    "            # get memory usage\n",
    "            float_size = np.dtype(np.float32).itemsize  # 4 bytes per float\n",
    "            memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "            memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "            print(f\"It uses {round(memory_usage_mb, 2)} MB\")\n",
    "\n",
    "            # test searcging speed\n",
    "            start = time.time()\n",
    "            for query in queries:\n",
    "                query_milvus(collection, query, top_k=5)\n",
    "            search_time = time.time() - start\n",
    "            print(f\"Search with chunk size {chunk} and overlap {overlap} completed in {round(search_time, 2)} seconds\")\n",
    "\n",
    "            # add results to resul dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, num_vectors, index_time, memory_usage_mb, search_time]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['database']=='chroma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>overlap</th>\n",
       "      <th>clean</th>\n",
       "      <th>embedding</th>\n",
       "      <th>chunks_processed</th>\n",
       "      <th>time_to_process</th>\n",
       "      <th>used_memory_mb</th>\n",
       "      <th>query_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>milvus</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>nomic-embed-text</td>\n",
       "      <td>415</td>\n",
       "      <td>12.192802</td>\n",
       "      <td>1.215820</td>\n",
       "      <td>17.138979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>milvus</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>nomic-embed-text</td>\n",
       "      <td>423</td>\n",
       "      <td>13.813842</td>\n",
       "      <td>1.239258</td>\n",
       "      <td>17.191782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>milvus</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>nomic-embed-text</td>\n",
       "      <td>637</td>\n",
       "      <td>15.134689</td>\n",
       "      <td>1.866211</td>\n",
       "      <td>17.259680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milvus</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "      <td>nomic-embed-text</td>\n",
       "      <td>669</td>\n",
       "      <td>17.386459</td>\n",
       "      <td>1.959961</td>\n",
       "      <td>16.320756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>milvus</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>nomic-embed-text</td>\n",
       "      <td>303</td>\n",
       "      <td>10.613719</td>\n",
       "      <td>0.887695</td>\n",
       "      <td>16.875774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database chunk_size overlap  clean         embedding chunks_processed  \\\n",
       "0   milvus        100       0   True  nomic-embed-text              415   \n",
       "1   milvus        100       0  False  nomic-embed-text              423   \n",
       "2   milvus        100      50   True  nomic-embed-text              637   \n",
       "3   milvus        100      50  False  nomic-embed-text              669   \n",
       "4   milvus        300       0   True  nomic-embed-text              303   \n",
       "\n",
       "   time_to_process  used_memory_mb  query_time  \n",
       "0        12.192802        1.215820   17.138979  \n",
       "1        13.813842        1.239258   17.191782  \n",
       "2        15.134689        1.866211   17.259680  \n",
       "3        17.386459        1.959961   16.320756  \n",
       "4        10.613719        0.887695   16.875774  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['database']=='milvus'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index().loc[35, 'chunks_processed'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('experiment_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS4300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
