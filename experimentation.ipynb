{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingest_redis import *\n",
    "from src.ingest_chroma import *\n",
    "from src.ingest_milvus import * \n",
    "from src.search import *\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataframe to track experiment results\n",
    "cols =  ['database', 'chunk_size', 'overlap', 'clean', 'embedding', 'chunks_processed', 'time_to_process', 'used_memory_mb', 'query_time']\n",
    "results = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_queries.txt', 'r') as file:\n",
    "\n",
    "    # Skip lines that don't contain actual queries (headers, empty lines) and extract example queries\n",
    "    queries = [line.strip() for line in file if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('##')]\n",
    "    queries = [q.split('. ', 1)[1] if '. ' in q else q for q in queries]\n",
    "    \n",
    "    # Print total count\n",
    "    print(f\"Total queries: {len(queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "db = 'redis'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # clear store before starting\n",
    "            clear_redis_store()\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            create_hnsw_index()\n",
    "            chunk_count = process_pdfs_redis(\"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "\n",
    "            # Get memory usage info\n",
    "            memory_info = redis_client.info('memory')\n",
    "            used_memory = memory_info['used_memory'] \n",
    "            used_memory_mb = used_memory / (1024 * 1024)\n",
    "\n",
    "            # test retrieval speed\n",
    "            start = time.time()\n",
    "            for query in queries:\n",
    "                query_redis(query)\n",
    "            to_search = time.time()- start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "embedding_size = 768\n",
    "db = 'chroma'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            collection = create_chroma_index(embedding_model)\n",
    "            collection, chunk_count = process_pdfs_chroma(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Index with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "            num_vectors = len(collection.get()[\"ids\"])  # Number of stored items\n",
    "            embedding_size = 768  # Adjust based on your embedding model\n",
    "            float_size = np.dtype(np.float32).itemsize  # 4 bytes per float\n",
    "\n",
    "            memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "            memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "            print(f\"It uses {memory_usage_bytes} MB\")\n",
    "\n",
    "            # test searching speed\n",
    "            start = time.time()\n",
    "            query_chroma(collection, queries)\n",
    "            to_search = time.time()-start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different chunk/overlap/clean combos for filling redis database\n",
    "embedding_model = 'nomic-embed-text'\n",
    "embedding_size = 768\n",
    "db = 'milvus'\n",
    "\n",
    "# loop through different options\n",
    "for chunk in [100,300,500,1000]:\n",
    "    for overlap in [0, 50, 100]:\n",
    "        if overlap >= chunk:\n",
    "            continue\n",
    "        for clean in [True, False]:\n",
    "\n",
    "            # create and fill redis store\n",
    "            start = time.time()\n",
    "            collection = create_milvus_collection(embedding_model)\n",
    "            collection, chunk_count = process_pdfs_milvus(collection, \"data/\", chunk_size=chunk, overlap=overlap, clean = clean)\n",
    "            to_fill = time.time() - start\n",
    "            print(f'Collection with chunk size {chunk} and overlap {overlap} created in {round(to_fill, 2)} seconds')\n",
    "\n",
    "            num_vectors = len(collection.get()[\"ids\"])  # Number of stored items\n",
    "            embedding_size = 768  # Adjust based on your embedding model\n",
    "            float_size = np.dtype(np.float32).itemsize  # 4 bytes per float\n",
    "\n",
    "            memory_usage_bytes = num_vectors * embedding_size * float_size\n",
    "            memory_usage_mb = memory_usage_bytes / (1024 * 1024)\n",
    "            print(f\"It uses {memory_usage_bytes} MB\")\n",
    "\n",
    "            # test searching speed\n",
    "            start = time.time()\n",
    "            query_milvus(collection, queries)\n",
    "            to_search = time.time()-start\n",
    "            print(f'Search with chunk size {chunk} and overlap {overlap} completed in {round(to_search, 2)} seconds')\n",
    "\n",
    "            # add results to result dataframe\n",
    "            new_row = [db, chunk, overlap, clean, embedding_model, chunk_count, to_fill, used_memory_mb, to_search]\n",
    "            results = pd.concat([results, pd.DataFrame([new_row], columns=cols)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['database']=='chroma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['database']=='chroma'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.reset_index().loc[35, 'chunks_processed'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('experiment_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS4300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
